{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.getenv(\"HOST\")\n",
    "port = os.getenv(\"PORT\")\n",
    "genai.configure(api_key = os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are having a Conversation with a human.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "chatbot:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template= template, input_variables=[\"chat_history\", \"human_input\"])\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model = \"gemini-pro\")\n",
    "output_parser = StrOutputParser()\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hi\")\n",
    "history.add_ai_message(\"Hi! How can i help you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama2\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are a chatbot who is having a conversation with a human.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Bot:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llmChain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt, \n",
    "    memory= memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain.run(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain.predict(human_input = \"I am fine. How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ( ChatPromptTemplate, \n",
    "                                HumanMessagePromptTemplate,\n",
    "                                MessagesPlaceholder)\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content = \"You are a chatbot having a conversation with human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder  (\n",
    "            variable_name = \"chat_history\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        )\n",
    "]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"llama2\")\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory= memory,\n",
    "    prompt=prompt, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm_chain.predict(human_input=\"Hi there my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import(HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate)\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = Ollama(model = \"llama2\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{human_input}\")\n",
    "    ]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key= \"chat_history\", return_messages=True)\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm, \n",
    "    memory = memory, \n",
    "    prompt = prompt,\n",
    "    verbose=True\n",
    ")\n",
    "llm_chain.predict(human_input = \"Hi there\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up until now the llm is only able to take single input. So to build a model with a capability to have multiple QnA. we do the following:\n",
    "1. Read the file to give it to the model to read the content.\n",
    "2. Split the texts\n",
    "3. Make embedings\n",
    "4. Similarity search from the database\n",
    "5. Template, Memory, Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from langchain.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "state_of_the_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts = texts,\n",
    "                              embedding= embeddings,\n",
    "                              metadatas = [{\"source\": i} for i in range(len(texts))]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Justice Breyer\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "\n",
    "You are a chatbot who is having a conversation with a human.\n",
    "Given the following extracted parts of a long document and a question, create a final answer.\n",
    "{context}\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "llm = Ollama(model = \"llama2\")\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"chat_history\", \"human_input\"], template= template)\n",
    "memory = ConversationBufferMemory(memory_key= \"chat_history\", input_key= \"human_input\")\n",
    "chain = load_qa_chain(memory= memory, prompt = prompt, chain_type= \"stuff\",llm =llm )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Justice Bryer\"\n",
    "chain({\"input_documents\": docs, \"human_input\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue further we need to learn more about Agents so now goto ../Agents/ to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.vectorstores import FAISS\n",
    "import google.generativeai as genai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel, Field, EmailStr\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_tagging_chain,create_tagging_chain_pydantic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model = Ollama(model = \"llama2\")\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=0)\n",
    "# model = ChatGoogleGenerativeAI(model = 'gemini-pro', temperature=0)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "template = \"\"\"\n",
    "### Instructions\n",
    "You are a virtual assistant for a company named Gojo and co. You are given \n",
    "context in which you should answer the queries from which is given between two back ticks.\\\n",
    "You are given queried by the user in triple backticks \\\n",
    "the user query with relevant answers and do not provide irrevant answers, STRICTLY.\\\n",
    "ANSWER WITHIN 50 WORDS.\n",
    "query: ```{query}```\n",
    "context: ``{context}``\n",
    "Provide the answers without backticks.\n",
    "Ask the user their Name, Email and PhoneNumber at the very beginning.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# database = FAISS.load_local(\"ConversationalFormDemo\",embeddings)\n",
    "\n",
    "class PersonalDetails(BaseModel):\n",
    "    first_name: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the first name of the user.\",\n",
    "    )\n",
    "    last_name: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the last name of the user.\",\n",
    "    )\n",
    "    email: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the email of the user\",\n",
    "    )\n",
    "\n",
    "chain = create_tagging_chain_pydantic(PersonalDetails,llm)\n",
    "\n",
    "# def get_user_input() -> PersonalDetails:\n",
    "#     first_name = input(\"Enter your first name: \")\n",
    "#     last_name = input(\"Enter your last name: \")\n",
    "#     age = input(\"Enter your Age: \")\n",
    "#     return PersonalDetails(first_name=first_name,last_name = last_name, age=age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='Ajeet', last_name='', email='')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_string = \"My last name is acharya and you can contact me at ajeetach@gmail.com.\"\n",
    "test_string_1 = \"my first name is Ajeet\"\n",
    "res = chain.run(test_string_1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='', last_name='', email='')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_123_personal_details = PersonalDetails(first_name=\"\",\n",
    "                                last_name=\"\",\n",
    "                                email=\"\",\n",
    ")\n",
    "user_123_personal_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_what_is_empty(user_personal_details):\n",
    "    ask_for = []\n",
    "    for field, value in user_personal_details.dict().items():\n",
    "        if value in [None, \"\", 0]:\n",
    "            print(f\"Field '{field} is empty.'\")\n",
    "            ask_for.append(f'{field}')\n",
    "    return ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'first_name is empty.'\n",
      "Field 'last_name is empty.'\n",
      "Field 'email is empty.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_name', 'last_name', 'email']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for = check_what_is_empty(user_123_personal_details)\n",
    "ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the response and adding it\n",
    "def add_non_empty_details(current_details: PersonalDetails, new_details: PersonalDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.dict().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.copy(update=non_empty_details)\n",
    "    return updated_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='Ajeet', last_name='', email='')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_123_personal_details = add_non_empty_details(user_123_personal_details, res)\n",
    "user_123_personal_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='Ajeet', last_name='acharya', email='ajeetach@gmail.com')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chain.run(test_string)\n",
    "user_123_personal_details = add_non_empty_details(user_123_personal_details, res)\n",
    "user_123_personal_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for = check_what_is_empty(user_123_personal_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you wee have all the details\n"
     ]
    }
   ],
   "source": [
    "# if ask_for does not have Empty strings\n",
    "if not ask_for:\n",
    "    print(\"Thank you wee have all the details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it in LLMCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='', last_name='', email='')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_details = PersonalDetails(first_name=\"\",\n",
    "                                last_name=\"\",\n",
    "                                email=\"\",\n",
    ")\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_info(ask_for = ['first_name','last_name', \"email\"]):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Below is are some things to ask the user for in a coversation way. \\\n",
    "        you should only ask one question at a time even if you don't get all the info \\\n",
    "        don't ask as a list! Don't greet the user! Don't say Hi.\\\n",
    "        Explain you need to get some info.\\\n",
    "        If the ask_for list is empty then thank \\\n",
    "        them and ask how you can help them \\n\\n \\\n",
    "        ### ask_for list: {ask_for}\"\n",
    "    )\n",
    "    \n",
    "    info_chain = LLMChain(llm = llm, prompt=first_prompt)\n",
    "    ai_chat= info_chain.run(ask_for = ask_for)\n",
    "    return ai_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_response(text_input, user_details):\n",
    "    chain = create_tagging_chain_pydantic(PersonalDetails, llm)\n",
    "    res = chain.run(text_input)\n",
    "    user_details = add_non_empty_details(user_details, res)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    return user_details, ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can I please have your first name?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='John', last_name='Doe', email='johndoe@example.com')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_input = \"Ok. My name is Ajeet\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_input = \"My name is Ajeet acharya\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_input = \"Contact me at Ajeet@gmail.com\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "REDIS_SERVER = os.getenv('REDIS_SERVER') or \"localhost\"\n",
    "cache = redis.Redis(REDIS_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.vectorstores import FAISS\n",
    "import google.generativeai as genai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel, Field, EmailStr\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_tagging_chain,create_tagging_chain_pydantic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model = Ollama(model = \"llama2\")\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=0)\n",
    "# model = ChatGoogleGenerativeAI(model = 'gemini-pro', temperature=0)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# database = FAISS.load_local(\"ConversationalFormDemo\",embeddings)\n",
    "\n",
    "class PersonalDetails(BaseModel):\n",
    "    first_name: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the first name of the user.\",\n",
    "    )\n",
    "    last_name: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the last name of the user.\",\n",
    "    )\n",
    "    email: str = Field(\n",
    "        ...,\n",
    "        description=\"This is the email of the user\",\n",
    "    )\n",
    "\n",
    "chain = create_tagging_chain_pydantic(PersonalDetails,llm)\n",
    "\n",
    "# test_string = \"My last name is acharya and you can contact me at ajeetach@gmail.com.\"\n",
    "# test_string_1 = \"my first name is Ajeet\"\n",
    "# print(chain.run(test_string_1))\n",
    "\n",
    "# def get_user_input() -> PersonalDetails:\n",
    "#     first_name = input(\"Enter your first name: \")\n",
    "#     last_name = input(\"Enter your last name: \")\n",
    "#     age = input(\"Enter your Age: \")\n",
    "#     return PersonalDetails(first_name=first_name,last_name = last_name, age=age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_what_is_empty(user_personal_details):\n",
    "    ask_for = []\n",
    "    for field, value in user_personal_details.dict().items():\n",
    "        if value in [None, \"\", 0]:\n",
    "            print(f\"Field '{field} is empty.'\")\n",
    "            ask_for.append(f'{field}')\n",
    "    return ask_for\n",
    "\n",
    "\n",
    "## checking the response and adding it\n",
    "def add_non_empty_details(current_details: PersonalDetails, new_details: PersonalDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.dict().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.copy(update=non_empty_details)\n",
    "    return updated_details\n",
    "\n",
    "\n",
    "def ask_for_info(ask_for):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Below is are some things to ask the user for in a coversation way. \\\n",
    "        you should only ask one question at a time even if you don't get all the info \\\n",
    "        don't ask as a list! Don't greet the user! Don't say Hi.\\\n",
    "        Explain you need to get some info.\\\n",
    "        If the ask_for list is empty then thank \\\n",
    "        them and ask how you can help them. Do not fill garbage values to the fields. If the user does not enter details do not fill it with \\\n",
    "            answers like\\n\\n \\\n",
    "        ### ask_for list: {ask_for}\"\n",
    "    )\n",
    "    \n",
    "    info_chain = LLMChain(llm = llm, prompt=first_prompt)\n",
    "    ai_chat= info_chain.run(ask_for = ask_for)\n",
    "    return ai_chat\n",
    "\n",
    "def filter_response(text_input, user_details):\n",
    "    chain = create_tagging_chain_pydantic(PersonalDetails, llm)\n",
    "    res = chain.run(text_input)\n",
    "    user_details = add_non_empty_details(user_details, res)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    \n",
    "    return user_details, ask_for\n",
    "\n",
    "\n",
    "def filter_response(text_input, user_details):\n",
    "    chain = create_tagging_chain_pydantic(PersonalDetails, llm)\n",
    "    res = chain.run(text_input)\n",
    "    user_details = add_non_empty_details(user_details, res)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    print(user_details)\n",
    "    return user_details, ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonalDetails(first_name='', last_name='', email='')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_details = PersonalDetails(first_name=\"\",\n",
    "                                last_name=\"\",\n",
    "                                email=\"\",\n",
    ")\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can I please get your first name?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PersonalDetails\nemail\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ask_for\u001b[38;5;241m=\u001b[39m  [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m text_input \u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(ques)\n\u001b[0;32m----> 4\u001b[0m user_details, ask_for \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_details\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ask_for)\n",
      "Cell \u001b[0;32mIn[81], line 44\u001b[0m, in \u001b[0;36mfilter_response\u001b[0;34m(text_input, user_details)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_response\u001b[39m(text_input, user_details):\n\u001b[1;32m     43\u001b[0m     chain \u001b[38;5;241m=\u001b[39m create_tagging_chain_pydantic(PersonalDetails, llm)\n\u001b[0;32m---> 44\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     user_details \u001b[38;5;241m=\u001b[39m add_non_empty_details(user_details, res)\n\u001b[1;32m     46\u001b[0m     ask_for \u001b[38;5;241m=\u001b[39m check_what_is_empty(user_details)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    508\u001b[0m         _output_key\n\u001b[1;32m    509\u001b[0m     ]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    513\u001b[0m         _output_key\n\u001b[1;32m    514\u001b[0m     ]\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/llm.py:104\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/llm.py:258\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the text of the top generated string.\u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllm_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerations\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/chains/llm.py:261\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         {\n\u001b[0;32m--> 261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[1;32m    263\u001b[0m         }\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[1;32m    265\u001b[0m     ]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/langchain/output_parsers/openai_functions.py:159\u001b[0m, in \u001b[0;36mPydanticOutputFunctionsParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    157\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_only:\n\u001b[0;32m--> 159\u001b[0m     pydantic_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_result\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m _result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/pydantic/main.py:549\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_raw\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/pydantic/main.py:526\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/llmProj/lib/python3.11/site-packages/pydantic/main.py:342\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PersonalDetails\nemail\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "\n",
    "ques = ask_for_info()\n",
    "ask_for=  ['first_name','last_name', \"email\"]\n",
    "text_input =input(ques)\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "print(ask_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'last_name is empty.'\n",
      "Field 'email is empty.'\n",
      "first_name='Ajeet' last_name='' email=''\n"
     ]
    }
   ],
   "source": [
    "ques = ask_for_info() \n",
    "text_input =input(ques)\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = PersonalDetails(first_name=\"\",\n",
    "                                last_name=\"\",\n",
    "                                email=\"\",\n",
    ")\n",
    "user_details\n",
    "ask_for=  ['first_name','last_name', \"email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'email is empty.'\n",
      "first_name='ajeet' last_name='acharya' email=''\n",
      "['email']\n",
      "first_name='ajeet' last_name='acharya' email='ajeet@gmail.com'\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define your functions and classes here\n",
    "\n",
    "# Initialize user_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for_info()\n",
    "text_input = \"Ok. My name is Ajeet\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")\n",
    "    print(user_details)\n",
    "    \n",
    "text_input = \"Ok. My name is Ajeet Acharya and please contact me at ajeet@gmail.com\"\n",
    "print(text_input)\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")\n",
    "    \n",
    "text_input = \"Ok. My name is Ajeet Acharya and please contact me at ajeet@gmail.com\"\n",
    "print(text_input)\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for_info()\n",
    "text_input =\"Ajeet Acharya is my name\"\n",
    "# user_details, ask_for = filter_response(text_input, user_details)\n",
    "# if ask_for:\n",
    "#     ai_response = ask_for_info(ask_for)\n",
    "#     input\n",
    "#     print(ai_response)\n",
    "# else:\n",
    "#     print(\"Everything gathered move to next phase\")\n",
    "# print(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = ask_for_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = input(ques)\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for_info()\n",
    "text_input = \"Ok. My name is Ajeet\"\n",
    "user_details, ask_for = filter_response(text_input, user_123_personal_details)\n",
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")\n",
    "text_input = \"Ok. My name is Ajeet Acharya and please contact me at ajeet@gmail.com\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print(\"Everything gethered move to next phase\")\n",
    "text_input = \"Ok. My name is Ajeet Acharya and please contact me at ajeet@gmail.com\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"Ok. My name is Ajeet Acharya and please contact me at ajeet@gmail.com\"\n",
    "user_details, ask_for = filter_response(text_input, user_details)\n",
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
